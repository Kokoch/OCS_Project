This repository contains the report and all the code related to the group project made by Gabriel SPADONI, Elliot KOCH and Xavier-Robert FRANÃ‡OIS under the supervision of Emmanuel Kieffer and Tobias FISCHBACH for the course "Optimisation for Computer Science" of the second semester of the Master in Computer Science at the University of Luxembourg.

=========================================

# General Information

The code was used on the HPC of the Uni.lu.
In order to run python, we used the python package <venv>.

(Some files are not used anymore. Also the folder structure should not be changed as it may break the scripts and prevent the pipelines from working properly)

When running a 'pipeline*.sh' as a batch, a 'slurm-*.out' file will be generated and will contain the code output generated during execution. 

---

# Folders Content:

- The [java] folder contains the script 'pipeline_java.sh' which must be used to launch the analysis.
It also contains the program 'matrix.java' which performs the matrix operation in java and then 'data_write_java.py' is used to store the output in temporary files which will go in [java/java/outputs]. In [java/java/outputs] you will find the 'to_csv.py' which is called at the end of the script execution to regroup the data from each individual execution in a single file 'perf_output_all.csv' which is our dataset. 

- The [NNergy/outputs] folder contains the results from the neural network training energy analysis. In there you will find the file 'NN_to_csv.py', which is in charge of regrouping the other temporary files 'NN_loss*.txt' and 'perf_output*.txt' into a single file 'perf_output_NN.csv' which is the dataset generated by tracking energy for each separate combination of the selected hyperparameters.

- The [python] folder contains the 'matrix_gen.py' taht was originally a file uniquely focusing on generating the matrix. The file 'matrix_op.py' originally only contained the matrix multiplication operation and the file 'matrix.py' is a fusion of both that we further adapted to the needs of the pipeline. 
It also contains in [python/outputs] the program 'data_write.py' used to add parameter information to the output in temporary files which will go in [python/outputs]. The 'to_csv.py' which is called at the end of the script execution regroups the data from each individual execution in a single file 'perf_output_all.csv' which is our dataset for python. 

- the [venv] folder may have been discarded because of its size (7Go) and contains the python virtual environment we used to run our python. See more in 'requirements.txt'.

# Files:

- 'requirements.txt' detail of the python venv config and packages. You can use pip install -r requirements.txt.

- 'Tutorial HPC.txt' gives a brief explanation of the commands for the HPC which uses Linux

- 'pipeline.sh' file can be run with a sbatch after reserving space on the node to compute the energy cost for the python programs.

- 'NN_dataset.csv' contains the output data from the energy analysis for each of the different hyperparameters used during the training of the neural network. It is automatically generated by the script 'measure_optimal_training.sh'

- 'measure_training.sh' measures the energy consumption of a single run of the file 'ocs_project_NN.py' which performs a unique neural network training (contains unnecessary code but can be reused to run other neural network for exemple). 

- 'measure_optimal_training.sh' merges the python dataset and the java datasetn, then iterates through all the hyperparameters value selected and measures the energy consumption for each run, then writes the additional info about the hyperparameters to the 'perf_output*.txt' file associated to the run with the help of the file 'data_write_NN.py'. Finally, it fetches all the information in the .txt files to generate the dataset 'perf_output_NN.csv' located in [NNergy/outputs].

- 'dataset_merge.py' fetches the datasets generated by the Python (java/java/outputs/perf_output_all.csv) and Java (python/outputs/perf_output_all.csv) energy analysis and merges them together as 'NN_dataset.csv'.

- 'compute_mono.sh' computes the energy cost of running a single matrix multiplication operation with the program 'matrix.py' located in [python]. This file was used at the beginning.

- 'data_write_NN.py' appends information to the perf output file. This is to help track the hyperparameters associated to each run in temporary file which will be transfered to the dataset for neural networks when called in the script.

- 'ocs_project_NN_optimization2.py' the latest version of the program used to train the neural network. This is the target of the energy evaluation. It is adapted to receive its parameters from the '.sh' script and they are written in a file together with the perf output.